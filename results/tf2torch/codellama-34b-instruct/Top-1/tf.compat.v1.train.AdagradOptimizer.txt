
PyTorch equivalent: torch.optim.Adagrad