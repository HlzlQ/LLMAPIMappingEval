  tf.keras.layers.PReLU() is a parametric rectified linear unit (PReLU) layer in TensorFlow. The equivalent API in MXNet is:

1. mxnet.gluon.nn.PReLU()
2. mxnet.ndarray.prelu()
3. mxnet.symbol.prelu()
4. mxnet.initializer.PReLU()
5. mxnet.optimizer.PReLU()